{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search methods for Retrieval and Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/santiagomvc/search_methods_intro/blob/main/search_methods.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "Tested with python 3.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy panda torch langchain-text-splitters sentence-transformers rank_bm25 faiss-cpu ranx ragatouille==0.0.8 llama-index==0.9.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from rank_bm25 import BM25Okapi\n",
    "from ranx import Qrels, Run, fuse, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility functions\n",
    "\n",
    "We define a simple text processing function. Possible improvements include tokenization, stemming, stopword removal, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_run(results_df, doc_id_col=\"chunk_id\"):\n",
    "    run_df = results_df.copy()\n",
    "    run = Run.from_df(\n",
    "        df=run_df,\n",
    "        q_id_col=\"query_id\",\n",
    "        doc_id_col=doc_id_col,\n",
    "        score_col=\"score\",\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_csv(\"https://raw.githubusercontent.com/santiagomvc/search_methods_intro/main/data/texts.csv\")\n",
    "queries_df = pd.read_csv(\"https://raw.githubusercontent.com/santiagomvc/search_methods_intro/main/data/queries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=64,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data and process chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = []\n",
    "chunk_ids = []\n",
    "chunk_texts = []\n",
    "for _, row in texts_df.iterrows():\n",
    "    doc_id = str(row[\"doc_id\"])\n",
    "    doc_chunk_texts = text_splitter.split_text(row[\"doc_text\"])\n",
    "    n_chunk_texts = len(doc_chunk_texts)\n",
    "    doc_chunk_ids = [f\"{doc_id}-{str(i)}\" for i in range(n_chunk_texts)]\n",
    "    # basic text processing\n",
    "    doc_chunk_texts = [text_preprocess(chunk) for chunk in doc_chunk_texts]\n",
    "    # save results\n",
    "    doc_ids.extend([doc_id] * n_chunk_texts)\n",
    "    chunk_ids.extend(doc_chunk_ids)\n",
    "    chunk_texts.extend(doc_chunk_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df = pd.DataFrame({\n",
    "    \"doc_id\": doc_ids,\n",
    "    \"chunk_id\": chunk_ids,\n",
    "    \"chunk_text\": chunk_texts,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25_tokenized_corpus = [chunk.split(\" \") for chunk in chunk_texts]\n",
    "bm25_index = BM25Okapi(bm_25_tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Transformers + Faiss Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentsim_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "sentsim_embeddings = sentsim_model.encode(chunk_texts)\n",
    "sentsim_embedding_size = sentsim_embeddings.shape[1]\n",
    "sentsim_index = faiss.IndexFlatL2(sentsim_embedding_size)\n",
    "sentsim_index.add(sentsim_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colbert + RAGatuille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 18:29:22] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":   # Required so ragatouille runs safely\n",
    "    try:\n",
    "        colbert_index = RAGPretrainedModel.from_index(\".ragatouille/colbert/indexes/index\")\n",
    "    except:\n",
    "        colbert_index = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "        colbert_index.index(\n",
    "            index_name=\"index\", \n",
    "            collection=chunk_texts, \n",
    "            document_ids=chunk_ids, \n",
    "            use_faiss=False,\n",
    "            max_document_length=1024,\n",
    "            split_documents=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(query_text, bm25_index=bm25_index, chunks_df=chunks_df):\n",
    "    # Preprocess query same as docs\n",
    "    query_text = text_preprocess(query_text)\n",
    "    # Transform query\n",
    "    tokenized_query = query_text.split(\" \")\n",
    "    # Search with bm25 index\n",
    "    doc_scores = bm25_index.get_scores(tokenized_query)\n",
    "    # Format as dataframe\n",
    "    bm25_df = chunks_df.copy()\n",
    "    bm25_df[\"score\"] = doc_scores\n",
    "    bm25_df = bm25_df.loc[bm25_df[\"score\"] > 0]\n",
    "    # Drop to get docs, no chunks\n",
    "    bm25_df = bm25_df.sort_values(\"score\", ascending=False)\n",
    "    bm25_df = bm25_df.drop_duplicates(subset=[\"doc_id\"], keep=\"first\")\n",
    "    # Return results\n",
    "    return bm25_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Vector Sentece Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentsim_search(query_text, sentsim_model=sentsim_model, sentsim_index=sentsim_index, chunks_df=chunks_df, k=5):\n",
    "    # Preprocess query same as docs\n",
    "    query_text = text_preprocess(query_text)\n",
    "    # Encode query\n",
    "    sentsim_query_emb = sentsim_model.encode(query_text).reshape(1,-1)\n",
    "    # Search with embedding\n",
    "    D, I = sentsim_index.search(sentsim_query_emb, k)\n",
    "    # Format as dataframe\n",
    "    sentsim_df = chunks_df.copy()\n",
    "    sentsim_df = sentsim_df.loc[I[0]]\n",
    "    sentsim_df[\"score\"] = D[0].astype(float)\n",
    "    # Drop to get docs, no chunks\n",
    "    sentsim_df = sentsim_df.sort_values(\"score\", ascending=False)\n",
    "    sentsim_df = sentsim_df.drop_duplicates(subset=[\"doc_id\"], keep=\"first\")\n",
    "    return sentsim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colbert_search(query_text, colbert_index=colbert_index, chunks_df=chunks_df, k=5):\n",
    "    # Preprocess query same as docs\n",
    "    query_text = text_preprocess(query_text)\n",
    "    # Run query\n",
    "    colbert_results = colbert_index.search(query_text, k=k)\n",
    "    # Save results as a df\n",
    "    colbert_df = pd.DataFrame(colbert_results)\n",
    "    colbert_df = colbert_df.rename({\"document_id\": \"chunk_id\"}, axis=1)\n",
    "    colbert_df = colbert_df.merge(chunks_df, how=\"left\", on=\"chunk_id\")\n",
    "    colbert_df = colbert_df[[\"doc_id\", \"chunk_id\", \"chunk_text\", \"score\"]]\n",
    "    # Drop to get docs, no chunks\n",
    "    colbert_df = colbert_df.sort_values(\"score\", ascending=False)\n",
    "    colbert_df = colbert_df.drop_duplicates(subset=[\"doc_id\"], keep=\"first\")\n",
    "    return colbert_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank Fusion (Min-Max Norm, CombMAX fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_search(query_text, fusion_norm=\"min-max\", fusion_method=\"max\", chunks_df=chunks_df):\n",
    "    runs = []\n",
    "    for search_fun in [bm25_search, sentsim_search, colbert_search]:\n",
    "        # Save results in Run format\n",
    "        run_df = search_fun(query_text)\n",
    "        run_df[\"query_id\"] = \"0\"   # query id is required for the run\n",
    "        # run_df[\"chunk_id\"] = run_df[\"chunk_id\"].astype(str)\n",
    "        run = build_run(run_df)\n",
    "        runs.append(run)\n",
    "    ## Combining runs\n",
    "    combined_run = fuse(\n",
    "        runs=runs,\n",
    "        norm=fusion_norm,\n",
    "        method=fusion_method,\n",
    "    )\n",
    "    ## Saving as dataframe\n",
    "    combined_df = combined_run.to_dataframe()\n",
    "    combined_df = combined_df.drop(\"q_id\", axis=1)\n",
    "    combined_df = combined_df.rename({\"doc_id\": \"chunk_id\"}, axis=1)\n",
    "    combined_df = combined_df.merge(chunks_df, how=\"left\", on=\"chunk_id\")\n",
    "    combined_df = combined_df[[\"doc_id\", \"chunk_id\", \"chunk_text\", \"score\"]]\n",
    "    # Drop to get docs, no chunks\n",
    "    combined_df = combined_df.sort_values(\"score\", ascending=False)\n",
    "    combined_df = combined_df.drop_duplicates(subset=[\"doc_id\"], keep=\"first\")\n",
    "    ## Return similar format to other responses\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_text, mode=\"bm25\"):\n",
    "    if mode==\"bm25\":\n",
    "        return bm25_search(query_text)\n",
    "    elif mode==\"sentsim\":\n",
    "        return sentsim_search(query_text)\n",
    "    elif mode==\"colbert\":\n",
    "        return colbert_search(query_text)\n",
    "    elif mode==\"combined\":\n",
    "        return combined_search(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluates search with labeled queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search(mode=\"bm25\", queries_df=queries_df):\n",
    "    # Preprocess df\n",
    "    queries_df[\"query_id\"] = queries_df[\"query_id\"].astype(str)\n",
    "    queries_df[\"doc_id\"] = queries_df[\"doc_id\"].astype(str)\n",
    "    queries_df.loc[queries_df[\"score\"] > 0, \"score\"] = 1   # Replace all positive scores with 1\n",
    "    # Create Qrel for evaluation\n",
    "    qrels = Qrels.from_df(\n",
    "        df=queries_df,\n",
    "        q_id_col=\"query_id\",\n",
    "        doc_id_col=\"doc_id\",\n",
    "        score_col=\"score\",\n",
    "    )\n",
    "\n",
    "    # Get search responses\n",
    "    unique_queries_df = queries_df[[\"query_id\", \"query_text\"]].drop_duplicates()\n",
    "    unique_queries_list = unique_queries_df.values.tolist()\n",
    "    responses_list = []\n",
    "    for query_id, query_text in unique_queries_list:\n",
    "        response_df = search(query_text, mode=mode)\n",
    "        response_df[\"query_id\"] = query_id\n",
    "        responses_list.append(response_df)\n",
    "\n",
    "    # Build run dataframe\n",
    "    run_df = pd.concat(responses_list)\n",
    "    run_df[\"doc_id\"] = run_df[\"doc_id\"].astype(str)\n",
    "    run = build_run(run_df, doc_id_col=\"doc_id\")\n",
    "\n",
    "    ## Evaluate run\n",
    "    metrics = evaluate(qrels, run, [\"f1\", \"mrr\"])\n",
    "    print(mode, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 {'f1': 0.6981481481481481, 'mrr': 0.9166666666666666}\n"
     ]
    }
   ],
   "source": [
    "evaluate_search(mode=\"bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Single Vector Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentsim {'f1': 0.6555555555555556, 'mrr': 0.8888888888888888}\n"
     ]
    }
   ],
   "source": [
    "evaluate_search(mode=\"sentsim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Colbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index index for the first time... This may take a few seconds\n",
      "[Jul 02, 18:29:35] #> Loading codec...\n",
      "[Jul 02, 18:29:35] #> Loading IVF...\n",
      "[Jul 02, 18:29:35] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 18:29:36] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 603.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 18:29:36] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 65.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 18:29:36] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 18:29:36] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . juneteenth, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2238, 17389,  3372,  2232,   102,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colbert {'f1': 0.7944444444444444, 'mrr': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluate_search(mode=\"colbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Rank Fusion (Min-Max Norm, CombMAX fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined {'f1': 0.6814814814814815, 'mrr': 0.8888888888888888}\n"
     ]
    }
   ],
   "source": [
    "evaluate_search(mode=\"combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: health\n",
      "Enter the search mode (bm25, sentsim, colbert, combined): combined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagovelez/anaconda3/envs/temp2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'doc_id': '16',\n",
       "  'chunk_id': '16-4',\n",
       "  'chunk_text': \"13 vacation rentals or short-term rentals as follows:\\n14 1. to protect the public's health and safety, including rules and\\n15 regulations related to fire and building codes, health and sanitation,\",\n",
       "  'score': 1.0},\n",
       " {'doc_id': '10',\n",
       "  'chunk_id': '10-48',\n",
       "  'chunk_text': '110 of social work, psychologist licensed by the board of psychology, or other licensed counseling\\n111 professional with appropriate experience and training, provided that any such individual makes progress',\n",
       "  'score': 1.0},\n",
       " {'doc_id': '17',\n",
       "  'chunk_id': '17-56',\n",
       "  'chunk_text': '41 (10) \"health care provider\" or \"provider\" means any person or entity li-\\n42 censed, certified, or otherwise authorized by law to administer health care\\n43 in the ordinary course of business or practice of a profession, including',\n",
       "  'score': 1.0},\n",
       " {'doc_id': '14',\n",
       "  'chunk_id': '14-76',\n",
       "  'chunk_text': '176 analysts, and other licensed health and behavioral positions, which may either be employed by the\\n177 school board or provided through contracted services.',\n",
       "  'score': 0.439275072516164},\n",
       " {'doc_id': '12',\n",
       "  'chunk_id': '12-42',\n",
       "  'chunk_text': '100 deduction for such taxable year for long-term health care insurance premiums paid by him.\\n101 11. contract payments to a producer of quota tobacco or a tobacco quota holder, or their spouses, as',\n",
       "  'score': 0.1682393388722695},\n",
       " {'doc_id': '13',\n",
       "  'chunk_id': '13-6',\n",
       "  'chunk_text': '19 conditions of employment of the workforce \\n20 \\n21 . however, no locality shall adopt any workplace rule, other than for the purposes of a\\n22 community services board or behavioral health authority as defined in § 37.2-100, that prevents an',\n",
       "  'score': 0.01917413339973471},\n",
       " {'doc_id': '5',\n",
       "  'chunk_id': '5-7',\n",
       "  'chunk_text': '18 \\n19 \\n20 \\n21 \\n22 \\n23 \\n24 \\n25 \\n26 \\n27 5. the mental and physical health of all individuals involved.\\n28 6. which parent is more likely to allow the child frequent,\\n29 meaningful and continuing contact with the other parent. this paragraph',\n",
       "  'score': 0.0}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text = input(\"Enter your query:\")\n",
    "search_mode = input(\"Enter the search mode (bm25, sentsim, colbert, combined):\")\n",
    "search(query_text, search_mode).to_dict(orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
